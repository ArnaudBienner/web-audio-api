
<!DOCTYPE html>
<html>
  <head>
    <title>Web Audio Processing: Use Cases and Requirements</title>
    <meta http-equiv='Content-Type' content='text/html;charset=utf-8'/>
    <style type="text/css" media="all">
      table {border-collapse: collapse; border: 1px solid #000; font: normal 80%/140% arial, helvetica, sans-serif; color: #555; background: #fff;}
      td, th {border: 1px dotted #bbb; padding:.5em 1em; font-size: x-small; width: 10em; }
      caption {padding: 0 0 .5em 0; text-align: left; font-size: 1; font-weight: 500; text-align: center; color: #666; background: transparent;}
      table a {padding: 1px; text-decoration: none; font-weight: bold; background: transparent;}
      table a:link {border-bottom: 1px dashed #ddd; color: #000;}
      table a:visited {border-bottom: 1px dashed #ccc; text-decoration: line-through; color: #808080;}
      table a:hover {border-bottom: 1px dashed #bbb; color: #666;}
      thead th, tfoot th {white-space: nowrap; border: 1px solid #000; text-align: center; color: black; background: #ddd;}
      tfoot td {border: 2px solid #000;}
      tbody { height: 300px; overflow: auto; }
      tbody th {color: #060606; }
      tbody th, tbody td {vertical-align: middle; text-align: center; }
    </style>
    <!-- 
      === NOTA BENE ===
      For the three scripts below, if your spec resides on dev.w3 you can check them
      out in the same tree and use relative links so that they'll work offline,
     -->
     <script src='http://www.w3.org/Tools/respec/respec-w3c-common' class='remove' async></script>
    <script class='remove'>
      var respecConfig = {
          // specification status (e.g. WD, LCWD, NOTE, etc.). If in doubt use ED.
          specStatus:           "ED",
          
          // the specification's short name, as in http://www.w3.org/TR/short-name/
          shortName:            "audioproc-reqs",

          // if your specification has a subtitle that goes below the main
          // formal title, define it here
          //subtitle   :  "an excellent document",

          // if you wish the publication date to be other than today, set this
          // publishDate:  "2009-08-06",

          // if the specification's copyright date is a range of years, specify
          // the start date here:
          copyrightStart: "2011",

          // if there is a previously published draft, uncomment this and set its YYYY-MM-DD date
          // and its maturity status
          // previousPublishDate:  "1977-03-15",
          // previousMaturity:  "WD",

          // if there a publicly available Editor's Draft, this is the link
          edDraftURI:           "https://dvcs.w3.org/hg/audio/raw-file/tip/ucr/Overview.html",

          // if this is a LCWD, uncomment and set the end of its review period
          // lcEnd: "2009-08-05",

          // if you want to have extra CSS, append them to this list
          // it is recommended that the respec.css stylesheet be kept
          extraCSS:             ["http://dev.w3.org/2009/dap/ReSpec.js/css/respec.css"],

          // editors, add as many as you like
          // only "name" is required
          editors:  [
              { name: "Joe Berkovitz", company: "Noteflight", companyURL: "http://www.noteflight.com/" },
              { name: "Olivier Thereaux", company: "British Broadcasting Corporation (BBC)", companyURL: "http://bbc.co.uk/" },
          ],

          // authors, add as many as you like. 
          // This is optional, uncomment if you have authors as well as editors.
          // only "name" is required. Same format as editors.

          //authors:  [
          //    { name: "Your Name", url: "http://example.org/",
          //      company: "Your Company", companyURL: "http://example.com/" },
          //],
          
          // name of the WG
          wg:           "Audio Working Group",
          
          // URI of the public WG page
          wgURI:        "http://www.w3.org/2011/audio/",
          
          // name (without the @w3c.org) of the public mailing to which comments are due
          wgPublicList: "public-audio",
          
          // URI of the patent status for this WG, for Rec-track documents
          // !!!! IMPORTANT !!!!
          // This is important for Rec-track documents, do not copy a patent URI from a random
          // document unless you know what you're doing. If in doubt ask your friendly neighbourhood
          // Team Contact.
          wgPatentURI:  "http://www.w3.org/2004/01/pp-impl/46884/status",
      };
    </script>
  </head>
  <body>
    <section id='abstract'>
      <p>This document introduces a series of scenarios and a list of requirements guiding the work of the W3C Audio Working Group in its development of a web API for processing and synthesis of audio on the web. </p>
      
    </section>
    <section>
    <h2>Introduction</h2>
    <p>What should the future web sound like? That was, in essence, the mission of the W3C Audio Working Group when it was chartered in early 2011 to “support the features required by advanced interactive applications including the ability to process and synthesize audio”. Bringing audio processing and synthesis capabilities to the Open Web Platform should allow developers to re-create well-loved audio software on the open web and add great sound to web games and applications; it may also enable web developers to reinvent the world of audio and music by making it more connected, linked and social.</p>

    <p>This document attempts to describe the scenarios considered by the W3C Audio Working Group in its work to define Web Audio technologies. Not intended to be a comprehensive list of things which the Web Audio standards will make possible, it nevertheless attempts to:</p>

<ul>
<li>document a number of key applications of audio which Web Audio standards should enable,</li>
<li>provide a basis for discussion on the desired architecture of Web Audio standards,</li>
<li>offer examples for early uses of the technology, which can then be used to gather feedback on the draft standard, and</li>
<li>extract technical and architectural requirements for the Web Audio APIs or libraries built upon it.</li>
</ul>

    </section>  
    <section>
      <h2>Web Audio Scenarios</h2>
      
      <p>This section will introduce a number of scenarios involving the use of Web Audio processing or synthesis technologies, and discuss implementation and architectural considerations.</p>
    
      <section>
      <h3>Video Chat Application</h3>
      <p>Three people have joined a three-way conversation through a web application. Each of them see the other two participants in split windows, and hear their voice in sync with the video.</p>
      <p>The application provides a simple interface to control the incoming audio and video of the other participants: at any time, the user can mute the incoming streams, control the overall sound volume, or mute themselves while continuing to send a live video stream through the application.</p>

      <p>Advanced controls are also available. In the "Audio" option panel, the user has the ability to adapt the incoming sound to their taste through a graphic equalizer interface, as well as a number of filters for voice enhancement, a feature which can be useful between people with hearing difficulties, in imperfect listening environments, or to compensate for poor transmission environments.</p> 
      
      <p>Another option allows the user to change the spatialization of the voices of their interlocutors; the default is a binaural mix matching the disposition of split-windows on the screen, but the interface makes it possible to reverse the left-right balance, or make the other participants appear closer or further apart.</p>
              
      <p>The makers of the chat applications also offer a "fun" version which allows users to distort (pitch, speed, other effects) their voice. They are considering adding the option to the default software, as such a feature could also be used to protect one participants' privacy in some contexts.</p>
            
      <h4>Notes and Implementation Considerations</h4>
      
      <ol>
        <li><p>The processing capabilities needed by this scenario include:</p>
          <ul>
            <li>Mixing and spatialization of several sound sources</li> 
            <li><em>Controlling the gain</em> (mute and volume control) of several audio sources</li>
            <li><em>Filtering</em> (EQ, voice enhancement)</li>
            <li>Modifying the pitch and speed of sound sources</li>
          </ul>
        </li>
        <li><p>This scenario is also a good example of the need for audio capture (from line in, internal microphone or other inputs). We expect this to be provided by <a href="http://www.w3.org/TR/html-media-capture/" title="HTML Media Capture">HTML Media Capture</a>.</p></li>
        <li><p>The <a href="http://tools.ietf.org/html/draft-ietf-rtcweb-use-cases-and-requirements-06#section-4.2.1">first scenario in WebRTC's Use Cases and Requirements document</a> has been a strong inspiration for this scenario. Most of the technology, described above should be covered by the <a href="http://www.w3.org/TR/webrtc/" title="WebRTC 1.0: Real-time Communication Between Browsers">Web Real-Time Communication API</a>. The scenario illustrates, however, the need to integrate audio processing with the handling of RTC streams, with a technical requirement for processing of the audio signal at both ends (capture of the user's voice and output of its correspondents' conversation).</p></li>
      </ol>
      
      </section>
      
      <section>
      <h3>3D game with music and convincing sound effects</h3>
      <p>A user is playing a 3D first-person adventure game on their mobile device. The game is built entirely using open web technologies, but includes rich, convincing sound.</p>
      <p>As soon as the user starts the game, a musical background starts, loops seamlessly, and transitions smoothly from one music track to another as the player enters a house.</p>
      <p>While walking in a corridor, the player can hear the muffled sound of a ticking grandfather's clock. Following the direction of the sound and entering a large hall, the sound of the clock becomes clear, reverberating in the large hall. At any time, the sound of the clock spatialized in real-time based on the position of the player's character in the room (relative to the clock) and the current camera angle.</p>
      <p>As the soundscape changes, bringing a more somber, scary atmosphere to the scene, the player equips a firearm. Suddenly,  a giant snake springs from behind a corner, its hissing becoming a little louder as the snake turns its head towards the player. The weapon fires at the touch of a key, and the player can hear the sound of bullets in near-perfect synchronization with the firing, as well as the sound of bullets ricocheting against walls. The sounds are played immediately after the player presses the key, but the action and video frame rate can remain smooth even when a lot of sounds (bullets being fired, echoing and ricocheting, sound of the impacts, etc) are played at the same time. The snake is now dead, and many flies gather around it, and around the player's character, buzzing and zooming in the virtual space of the room.</p>

      <h4>Notes and Implementation Considerations</h4>
      
      <ol>
        <li><p>Developing the soundscape for a game as the one described above can benefit from a <em>modular, node based approach</em> to audio processing. In our scenario, some of the processing needs to happen for a number of sources at the same time (e.g room effects) while others (e.g mixing and spatialization) need to happen on a per-source basis. A graph-based API makes it very easy to envision, construct and control the necessary processing architecture, in ways that would be possible with other kinds of APIs, but more difficult to implement. The fundamental <code>AudioNode</code> construct in the Web Audio API supports this approach.</p></li>
        <li><p>While a single looping music background can be created today with the <a href="http://www.w3.org/TR/html5/the-audio-element.html#the-audio-element" title="4.8.7 The audio element &#8212; HTML5">HTML5 &lt;audio&gt; element</a>, the ability to transition smoothly from one musical background to another requires additional capabilities that are found in the Web Audio API including <em>sample-accurate playback scheduling</em> and <em>automated cross-fading of multiple sources</em>. Related API features include <code>AudioBufferSourceNode.noteOn()</code> and <code>AudioParam.setValueAtTime()</code>.</p></li>
        <li><p>The scenario illustrates many aspects of the creation of a credible soundscape. The game character is evolving in a virtual three-dimensional environment and the soundscape is at all times spatialized: a <em>panning model</em> can be used to spatialize sound sources in the game (<code>AudioPanningNode</code>); <em>obstruction / occlusion</em> modelling is used to muffle the sound of the clock going through walls, and the sound of flies buzzing around would need <em>Doppler Shift</em> simulation to sound believable (also supported by <code>AudioPanningNode</code>).  The listener's position is part of this 3D model as well (<code>AudioListener</code>).</p></li>
        <li><p>As the soundscape changes from small room to large hall, the game benefits from the <em>simulation of acoustic spaces</em>, possibly through the use of a <em>convolution engine</em> for high quality room effects as supported by <code>ConvolverNode</code> in the Web Audio API.</p></li> 
        <li><p>Many sounds in the scenario are triggered by events in the game, and would need to be played with low latency. The sound of the bullets as they are fired and ricochet against the walls, in particular, illustrate a requirement for <em>basic polyphony</em> and <em>high-performance playback and processing of many sounds</em>. These are supported by the general ability of the Web Audio API to include many sound-generating nodes with independent scheduling and high-throughput native algorithms.</p></li>
      </ol>


    </section>
    
    <section>

      <h3>Online music production tool</h3>
      
      <p>A user creates a musical composition from audio media clips using a web-based Digital Audio Workstation (DAW) application.</p>
      
      <p>Audio "clips" are arranged on a timeline representing multiple tracks of audio.  Each track's volume, panning, and effects
      may be controlled separately.  Individual tracks may be muted or soloed to preview various combination of tracks at a given moment.
      Audio effects may be applied per-track as inline (insert) effects.  Additionally, each track can send its signal to one or
      more global send effects which are shared across tracks.  Sub-mixes of various combinations of tracks can be made, and a final
      mix bus controls the overall volume of the mix, and may have additional insert effects.</p>
      
      <p>Insert and send effects include dynamics compressors (including multi-band), extremely high-quality reverberation, filters such as parametric, low-shelf, high-shelf, graphic EQ, etc.  Also included are various kinds of delay effects such as ping-pong delays, and BPM-synchronized delays with feedback.  Various kinds of time-modulated effects are available such as chorus, phasor, resonant filter sweeps, and BPM-synchronized panners.  Distortion effects include subtle tube simulators, and aggressive bit decimators.  Each effect has its own UI for adjusting its parameters.  Real-time changes to the parameters can be made (e.g. with a mouse) and the audible results heard with no perceptible lag.</p>
      
      <p>Audio clips may be arranged on the timeline with a high-degree of precision (with sample accurate playback).  Certain clips may be repeated loops containing beat-based musical material, and are synchronized with other such looped clips according to a certain musical tempo.  These, in turn, can be synchronized with sequences controlling real-time synthesized playback.  The values of volume, panning, send levels, and each parameter of each effect can be changed over time, displayed and controlled through a powerful UI dealing with automation curves.  These curves may be arbitrary and can be used, for example, to control volume fade-ins, filter sweeps, and may be synchronized in time with the music (beat synchronized).</p>
      
      <p>Visualizers may be applied for technical analysis of the signal.  These visualizers can be as simple as displaying the signal level in a VU meter, or more complex such as real-time frequency analysis, or L/R phase displays.</p>
      
      <p>The actual audio clips to be arranged on the timeline are managed in a library of available clips.  These can be searched and sorted in a variety of ways and with high-efficiency.  Although the clips can be cloud-based, local caching offers nearly instantaneous access and glitch-free playback.</p>
      
      <p>The final mix may be rendered at faster than real-time and then uploaded and shared with others.  The session representing the clips, timeline, effects, automation, etc. may also be shared with others for shared-mixing collaboration.</p>
      
      
      <h4>Notes and Implementation Considerations</h4>
      <ol>
        <li><p>This scenario details the large number of feature requirements typically expected of professional audio software or hardware. It encompasses many advanced audio control capabilities such as filtering, effects, dynamics compression and control of various audio parameters.</p></li>
        
        <li><p>Building such an application may only be reasonably possible if the technology enables the control of audio with acceptable performance, in particular for <em>real-time processing</em> and control of audio parameters and <em>sample accurate scheduling of sound playback</em>. Because performance is such a key aspect of this scenario, it should probably be possible to control the buffer size of the underlying Audio API: this would allow users with slower machines to pick a larger buffer setting that does not cause clicks and pops in the audio stream.</p></li>
        
        <li><p>The ability to visualise the samples and their processing benefits from <em>real-time time-domain and frequency analysis</em>, as supplied by the Web Audio API's <code>RealtimeAnalyzerNode</code>.</p></li>
        <li><p> Clips must be able to be loaded into memory for fast playback. The Web Audio API's <code>AudioBuffer</code> and <code>AudioBufferSourceNode</code> interfaces address this requirement.</p></li>
        <li><p>The ability to schedule both audio clip playback and effects parameter value changes in advance is essential to support automated mixdown</p></li>
        <li><p> To export an audio file, the audio rendering pipeline must be able to yield buffers of sample frames directly, rather than being forced to an audio device destination. Built-in codecs to translate these buffers to standard audio file output formats are also desirable.</p></li>
        <li><p>Typical per-channel effects such as panning, gain control, compression and filtering must be readily available in a native, high-performance implementation.</p></li>
        <li><p>Typical master bus effects such as room reverb must be readily available. Such effects are applied to the entire mix as a final processing stage. A single <code>ConvolverNode</code> is capable of simulating a wide range of room acoustics.</p></li>
      </ol>


    </section>
    
    <section>
      <h3>Online radio broadcast</h3>
      
      <p>A web-based online radio application supports one-to-many audio broadcasting on various channels.  For any one broadcast channel it exposes three separate user interfaces on different pages. One interface is used by the broadcaster controlling a radio show on the channel. A second interface allows invited guests to supply live audio to the show. The third interface is for the live online audience listening to the channel.</p>

      <p>The broadcaster interface supports live and recorded audio source selection as well as mixing of those sources.  Audio sources include:</p>
<ul>
  <li>any local microphone</li>
  <li>prerecorded audio such as jingles or tracks from music libraries</li>
  <li>a remote microphone for a remote guest</li>
</ul>

      <p>A simple mixer lets the broadcaster control the volume, pan and effects processing for each local or remote audio source, blending them into a single stereo output mix that is broadcast as the show's content.  Indicators track the level of each active source. This mixer also incorporates some automatic features to make the broadcaster's life easier, including ducking of prerecorded audio sources when any local or remote microphone source is active.  Muting(unmuting) of sources causes an automatic fast volume fade-out(in) to avoid audio transients. The broadcaster can hear a live monitor mix through headphones, with an adjustable level for monitoring their local microphone.</p>

      <p>The application is aware of when prerecorded audio is playing in the mix, and each audio track's descriptive metadata is shown to the audience in synchronization with what they are hearing.</p>

      <p>The guest interface supports a single live audio source from a choice of any local microphone.</p>

      <p>The audience interface delivers the channel's broadcast mix, but also offers basic volume and EQ control plus the ability to pause/rewind/resume the live stream. Optionally, the user can slow down the content of the audio without changing its pitch, for example to aid in understanding a foreign language.</p>
  
      <h4>Notes and Implementation Considerations</h4>
      <ol>
        <li><p>As with the Video Chat Application scenario, streaming and local device discovery and access within this scenario are handled by the <a href="http://www.w3.org/TR/webrtc/" title="WebRTC 1.0: Real-time Communication Between Browsers">Web Real-Time Communication API</a>. The local audio processing in this scenario highlights the requirement that <em>RTC streams and Web Audio be tightly integrated</em>. Incoming MediaStreams must be able to be exposed as audio sources, and audio destinations must be able to yield an outgoing RTC stream. For example, the broadcaster's browser employs a set of incoming MediaStreams from microphones, remote participants, etc., locally processes their audio through a graph of <code>AudioNodes</code>, and directs the output to an outgoing MediaStream representing the live mix for the show.</p></li>
        <li><p>Building this application requires the application of <em>gain control</em>, <em>panning</em>, <em>audio effects</em> and <em>blending</em> of multiple <em>mono and stereo audio sources</em> to yield a stereo mix. Some relevant features in the API include <code>AudioGainNode</code>, <code>ConvolverNode</code>, <code>AudioPannerNode</code>.</p></li>
        
        <li><p><em>Noise gating</em> (suppressing output when a source's level falls below some minimum threshold) is highly desirable for microphone inputs to avoid stray room noise being included in the broadcast mix. This could be implemented as a custom algorithm using a <code>JavaScriptAudioNode</code>.</p></li>
        <li><p>To drive the visual feedback to the broadcaster on audio source activity and to control automatic ducking, this scenario needs a way to easily <em>detect the time-averaged signal level</em> on a given audio source.</p></li>
        <li><p>Ducking affects the level of multiple audio sources at once, which implies the ability to associate a single <em>dynamic audio parameter</em> to the gain associated with these sources' signal paths.  The specification's <code>AudioGain</code> interface provides this.</p></li>
        <li><p>Smooth muting requires the ability to <em>smoothly automate gain changes</em> over a time interval, without using browser-unfriendly coding techniques like tight loops or high-frequency callbacks. The <em>parameter automation</em> features associated with <code>AudioParam</code> are useful for this kind of feature.</p></li>
        <li><p>Pausing and resuming the show on the audience side implies the ability to <em>buffer data received from audio sources</em> in the processing graph, and also to <em>send buffered data to audio destinations</em>.</p></li>
        <li><p>The functionality for audio speed changing, a custom algorithm, requires the ability to <em>create custom audio transformations</em> using a browser programming language (e.g. <code>JavaScriptAudioNode</code>). When audio delivery is slowed down, audio samples will have to be locally buffered by the application up to some allowed limit, since they continue to be delivered by the incoming stream at a normal rate.</p></li>
        <li><p>There is a standard way to access a set of <em>metadata properties for media resources</em> with the following W3C documents:
          <ul><li><p> <a href="http://www.w3.org/TR/mediaont-10/" title="http://www.w3.org/TR/mediaont-10/">Ontology for Media Resources 1.0</a>. This document defines a core set of metadata properties for media resources, along with their mappings to elements from a set of existing metadata formats.
          </p></li><li><p> <a href="http://www.w3.org/TR/mediaont-api-1.0/" title="http://www.w3.org/TR/mediaont-api-1.0/">API for Media Resources 1.0</a>. This API provides developers with a convenient access to metadata information stored in different metadata formats. It provides means to access the set of metadata properties defined in the Ontology for Media Resources 1.0 specification. 
          </p></li></ul>
        </p></li>
      </ol>
    </section>
      
      <section>
      
      
      <h3>Music Creation Environment with Sampled Instruments</h3>
      <p>A user is employing a web-based application to create and edit a musical composition with live synthesized playback. The user interface for composing can take a number of forms including conventional Western notation and a piano-roll style display. The document can be sonically rendered on demand as a piece of music, <em>i.e.</em> a series of precisely timed, pitched and modulated audio events (notes).
      <p>The user occasionally stops editing and wishes to hear playback of some or all of the score they are working on to take stock of their work. At this point the program performs sequenced playback of some portion of the document. Some simple effects such as instrument panning and room reverb are also applied for a more realistic and satisfying effect.</p>
      <p>Compositions in this editor employ a set of instrument samples, i.e. a pre-existing library of recorded audio snippets. Any given snippet is a brief audio recording of a note played on an instrument with some specific and known combination of pitch, dynamics and articulation. The combinations in the library are necessarily limited in number to avoid bandwidth and storage overhead. During playback, the editor must simulate the sound of each instrument playing its part in the composition.  This is done by transforming the available pre-recorded samples from their original pitch, duration and volume to match the characteristics prescribed by each note in the composed music.  These per-note transformations must also be scheduled to be played at the times prescribed by the composition.</p>
      <p>During playback a moving cursor indicates the exact point in the music that is being heard at each moment.</p>
      <p>At some point the user exports an MP3 or WAV file from the program for some other purpose. This file contains the same audio rendition of the score that is played interactively when the user requested it earlier.</p>

      <h4>Notes and Implementation Considerations</h4>
      <ol>
        <li><p> Instrument samples must be able to be loaded into memory for fast processing during music rendering. These pre-loaded audio snippets must have a one-to-many relationship with objects in the Web Audio API representing specific notes, to avoid duplicating the same sample in memory for each note in a composition that is rendered with it. The API's <code>AudioBuffer</code> and <code>AudioBufferSourceNode</code> interfaces address this requirement.</p></li>
        <li><p>It must be possible to schedule large numbers of individual events over a long period of time, each of which is a transformation of some original audio sample, without degrading real-time browser performance. A graph-based approach such as that in the Web Audio API makes the construction of any given transformation practical, by supporting simple recipes for creating subgraphs built around a sample's pre-loaded <code>AudioBuffer</code>.  These subgraphs can be constructed and scheduled to be played in the future. In one approach to supporting longer compositions, the construction and scheduling of future events can be kept "topped up" via periodic timer callbacks, to avoid the overhead of creating huge graphs all at once.</p></li>
        <li><p>A given sample must be able to be arbitrarily transformed in pitch and volume to match a note in the music. <code>AudioBufferSourceNode</code>'s <code>playbackRate</code> attribute provides the pitch-change capability, while <code>AudioGainNode</code> allows the volume to be adjusted.</p></li>
        <li><p>A given sample must be able to be arbitrarily transformed in duration (without changing its pitch) to match a note in the music. <code>AudioBufferSourceNode</code>'s looping parameters provide sample-accurate start and end loop points, allowing a note of arbitrary duration to be generated even though the original recording may be brief.</p></li>
        <li><p>Looped samples by definition do not have a clean ending. To avoid an abrupt glitchy cutoff at the end of a note, a gain and/or filter envelope must be applied. Such envelopes normally follow an exponential trajectory during key time intervals in the life cycle of a note. The <code>AudioParam</code> features of the Web Audio API in conjunction with <code>AudioGainNode</code> and <code>BiquadFilterNode</code> support this requirement.</p></li>
        <li><p> It is necessary to coordinate visual display with sequenced playback of the document, such as a moving cursor or highlighting effect applied to notes. This implies the need to programmatically determine the exact time offset within the performance of the sound being currently rendered through the computer's audio output channel. This time offset must, in turn, have a well-defined relationship to time offsets in prior API requests to schedule various notes at various times. The API provides such a capability in the <code>AudioContext.currentTime</code> attribute.</p></li>
        <li><p> To export an audio file, the audio rendering pipeline must be able to yield buffers of sample frames directly, rather than being forced to an audio device destination. Built-in codecs to translate these buffers to standard audio file output formats are also desirable.</p></li>
        <li><p>Typical per-channel effects such as stereo pan control must be readily available. Panning allows the sound output for each instrument channel to appear to occupy a different spatial location in the output mix, adding greatly to the realism of the playback.  Adding and configuring one of the Web Audio API's <code>AudioPannerNode</code> for each channel output path provides this capability.</p></li>
        <li><p>Typical master bus effects such as room reverb must be readily available. Such effects are applied to the entire mix as a final processing stage. A single <code>ConvolverNode</code> is capable of simulating a wide range of room acoustics.</p></li>
      </ol>

    </section>

    <section>
      <h3>Connected <abbr title="Disc Jockey">DJ</abbr> booth</h3>
      
      <p>A popular DJ is playing a live set, using a popular web-based DJ software. The web application allows her to perform both in the club where she is mixing, as well as online, with tens of thousands joining live to enjoy the set.</p>
    
      <p>The DJ-deck web interface offers the typical features of decks and turntables. While a first track is playing and its sound sent to both the sound system in the club and streamed to the web browsers of fans around the world, the DJ would be able to quickly select several other track, play them through headphones without affecting the main audio output of the application, and match them to the track currently playing through a mix of pausing, skipping forward or back and pitch/speed change. The application helps automate a lot of this work: by measuring the beat of the current track at 125BPM and the one of the chosen next track at 140 BPM, it can automatically slow down the second track, and even position it to match the beats of the one currently playing. </p>

      <p>Once the correct match is reached, The DJ would be able to start playing the track in the main audio output, either immediately or by slowly changing the volume controls for each track. She uses a cross fader to let the new song blend into the old one, and eventually goes completely across so only the new song is playing. This gives the illusion that the song never ended.</p>

      <p>At the other end, fans listening to the set would be able to watch a video of the DJ mixing, accompanied by a graphic visualization of the music, picked from a variety of choices: spectrum analysis, level-meter view or a number of 2D or 3D abstract visualisations displayed either next to or overlaid on the DJ video.</p>

      <h4>Notes and Implementation Considerations</h4>
      <ol>
        <li>As in many other scenarios in this document, it is expected that APIs such as the <a href="http://www.w3.org/TR/webrtc/" title="WebRTC 1.0: Real-time Communication Between Browsers">Web Real-Time Communication API</a> will be used for the streaming of audio and video across a number of clients.</li>
        <li>
          <p>One of the specific requirements illustrated by this scenario is the ability to seamlessly switch audio destinations (in this case: switching from listening on headphones to streaming sound output to a variety of local and connected clients) while retaining the exact state of playback and processing of a source. This may not be easy to achieve in the current Web Audio API draft, where a given <code>AudioContext</code> can only use one <code>AudioDestinationNode</code> as destination.</p> 
        </li>
        <li>This scenario makes heavy usage of audio analysis capabilities, both for automation purposes (beat detection and beat matching) and visualization (spectrum, level and other abstract visualization modes).</li>
        <li>The requirement for pitch/speed change are not currently covered by the Web Audio API's native processing nodes. Such processing would probably have to be handled with custom processing nodes.</li>
      </ol>
    </section>
      
    <section>
      <h3>Playful sonification of user interfaces</h3>

      <p>A child is visiting a social website designed for kids. The playful, colorful HTML interface is accompanied by sound effects played as the child hovers or clicks on some of the elements of the page. For example, when filling in a form the sound of a typewriter can be heard as the child types in the form field. Some of the sounds are spatialized and have a different volume depending on where and how the child interacts with the page. When an action triggers a download visualised with a progress bar, a gradually rising pitch sound accompanies the download and another sound (ping!) is played when the download is complete.</p>

      <h4>Notes and Implementation Considerations</h4>
      <ol>
        <li><p>Although the web UI incorporates many sound effects, its controls are embedded in the site's pages using standard web technology such as HTML form elements and CSS stylesheets. JavaScript event handlers may be attached to these elements, causing graphs of <code>AudioNodes</code> to be constructed and activated to produce sound output.</p></li>
        <li><p>Modularity, spatialization and mixing play an important role in this scenario, as for the others in this document.</p></li>
        <li><p>Various effects can be achieved through programmatic variation of these sounds using the Web Audio API. The download progress could smoothly vary the pitch of an <code>AudioBufferSourceNode</code>'s <code>playbackRate</code> using an exponential ramp function, or a more realistic typewriter sound could be achieved by varying an output filter's frequency based on the keypress's character code.</p></li>
        <li><p>In a future version of CSS, stylesheets may be able to support simple types of sonification, such as attaching a "typewriter key" sound to an HTML <code>textarea</code> element or a "click" sound to an HTML <code>button</code>. These can be thought of as an extension of the visual skinning concepts already embodied by style attributes such as <code>background-image</code>.</p></li>
    </section>
    
    <section>
      <h3>Podcast on a flight</h3>

      <p>A user is subscribed to a podcast, and has previously
      downloaded an audio book on his device using the podcast's
      web-based application.  The audio files are stored locally on
      the device, giving simple and convenient access to episodic
      content whenever the user wishes to listen.</p>

      <p>The user is sitting in an airplane for a 2-hour flight. The user opens 
      the podcast application in his HTML browser and sees that the episode he has 
      selected lasts 3 hours. The application offers a speed-up feature that allows
      the speech to be delivered at a faster than normal speed without pitch distortion
      ("chipmunk voices"). He sets the audition time to 
      2 hours in order to finish the audio book before landing. He also sets the sound
      control in the application to "Noisy Environment", causing the sound to be equalized
        for greatest intelligibility in a noisy setting such as an airplane.</p>

      <h4>Notes and Implementation Considerations</h4>
      <ol>
        <li><p>Local audio can be downloaded, stored and retrieved using the <a href="http://www.w3.org/TR/FileAPI/">HTML File API</a>.</p></li>
        <li><p>This scenario requires a special audio transformation that can compress the duration of speech
            without affecting overall timbre and intelligibility. In the Web Audio API this could be accomplished through
            attaching custom processing code to a <code>JavaScriptAudioNode</code>.</p></li>
        <li><p>The "Noisy Environment" setting could be accomplished through equalization features in the Web Audio API such as <code>BiquadFilterNode</code> or <code>ConvolverNode</code>.</p></li>
      </ol>
    </section>
        
    <section>
      <h3>Soundtrack and sound effects in a video editing tool </h3>
      <p>A person is using an online video editing tool to modify the soundtrack of a video.  The editor extracts the existing recorded vocals from the video stream, modifies the levels and performs other modifications of the audio stream.  She also adds several songs, including a orchestral background and pop songs, at different parts of the soundtrack.  She also adds several Foley effects (footsteps, doors opening and closing, etc.).  While editing, the audio and video playback are synced to allow the editor to insert audio samples at the right time. As the length of one of the songs is slightly different from the video segment she is matching it with, she can synchronize the two by slightly speeding up or slowing down the audio track. The final soundtrack is mixed down into the final soundtrack, added to the video as a replacement for the original audio track, and synced with the video track.
      </p>

    </section>
    
    <section>


      <h3>Web-based guitar practice service </h3>
      <p>A serious guitar player uses a web-based tool to practice a new tune. Connecting a  USB microphone and a pair of headphones to their computer, the guitarist is able to tune an acoustic guitar using a graphical interface, set a metronome to keep the tempo then start recording a practice session.
      </p><p>The audio input from the microphone is automatically analysed to detect whether the musician is keeping a regular beat. The music played during the session is recorded and can be saved to a variety of file formats locally or on the online service where others can replay, comment on the performance and annotate any section to help the musician improve technique and delivery.
      </p>
      
      </section>
      
      <section>
      
  
      
      <h3>User Control of Audio </h3>
      <p>A programmer wants to create a browser extension to allow the user to control the volume of audio on a per-tab basis, or to kill any audio playing completely, in a way that takes care of garbage collection.
      </p>
    </section>
    
    <section>


      <h3>Video commentary </h3>
      <p>The director of a video wants to add audio commentary to explain his creative process, and invites other people involved in the making of the video to do the same. His production team also prepares an audio description of the scenes to make the work more accessible to people with sight disabilities.
      </p><p>The video, displayed in a HTML web page, can be played with its original audio track (embedded in the video container) or with any of the audio commentary tracks loaded from a different source and synchronised with the video playback. When there's audio on the commentary track, the main track volume is reduced (ducked) gradually and smoothly brought back to full volume when the commentary / description track is silent. The visitor can switch between audio tracks on the fly, without affecting the video playback. Pausing the video playback also pauses the commentary track, which then remains in sync when playback resumes.
      </p><p><br />
      </p>
      <h4>UC15 — Notes </h4>
      <p>In discussions about this use case, some people in the group expressed a wish that this use case illustrate a need for the audio processing API to work well with the HTML5 <a href="http://dev.w3.org/html5/spec/media-elements.html#mediacontroller" title="http://dev.w3.org/html5/spec/media-elements.html#mediacontroller">MediaController</a>  interface. As this is an implementation question, this requirement has been kept out of the use case text itself, but the need to work with the HTML5 interface is duly noted.
      </p><p>A <a href="http://people.mozilla.org/~roc/stream-demos/video-with-extra-track-and-effect.html" title="http://people.mozilla.org/~roc/stream-demos/video-with-extra-track-and-effect.html">demo of this particular use case</a> using the MediaStream processing API is available.
      </p>
      
      </section>
    
    </section>
      
      <section>
    
      <h2>Requirements </h2>
      <section>
      <h3>Sources of audio</h3>
      
      <p>The Audio Processing API can operate on a number of sources of audio: </p>
      
      <ul>
        <li>  a DOM Element can be a source: HTML &lt;audio&gt; elements (with both remote and local sources)</li>
        <li> memory-resident PCM data can be a source: Individual memory-resident “buffers” of PCM audio data which are not associated with &lt;audio&gt; elements</li>
        <li> programmatically calculated data can be a source: on-the-fly generation of audio data</li>
        <li> devices can act as a source: Audio that has been captured from devices - microphones, instruments etc.</li>
        <li> remote peer can act as a source: Source from a remote peer (e.g. a WebRTC source) </li>
      </ul>
      
      
      <h4>Support for primary audio file formats </h4>
      <p>Sources of audio can be compressed or uncompressed, in typical standard formats found on the Web and in the industry (e.g. MP3 or WAV)</p>
      
      <h4>One source, many sounds </h4>
      <p>It should be possible to load a single source of sound and instantiate it in multiple, overlapping occurrences without reloading that are mixed together.</p>
          
      <h4>Playing / Looping  sources of audio </h4>
      <p>A subrange of a source can be played. It should be possible to start and stop playing a source of audio at any desired offset within the source. This would then allow the source to be used as an audio sprite. 
      See: <a href="http://remysharp.com/2010/12/23/audio-sprites/" class="external free" title="http://remysharp.com/2010/12/23/audio-sprites/">http://remysharp.com/2010/12/23/audio-sprites/</a>
      And: <a href="http://lists.w3.org/Archives/Public/public-audio/2012JanMar/0006.html" class="external free" title="http://lists.w3.org/Archives/Public/public-audio/2012JanMar/0006.html">http://lists.w3.org/Archives/Public/public-audio/2012JanMar/0006.html</a></p>
      <p>A source can be looped. It should be possible to loop memory-resident sources. It should be possible to loop on a whole-source and intra-source basis, or to play the beginning of a sound leading into a looped segment.</p>
      
      <h4>Capture of audio from microphone, line in, other inputs </h4>
      <p>Audio from a variety of sources, including line in and microphone input, should be made available to the Web Audio API for processing.</p>
      
      <h4>Adding effects to the audio part of a video stream, and keep it in sync with the video playback </h4>
      <p>The API should have access to the audio part of video streams being played in the browser, and it should be possible to add effects to it and output the result in real time during video playback, keeping audio and video in sync.</p>
      
      <h4>Sample-accurate scheduling of playback </h4>
      <p>In the case of memory-resident sources it should be possible to trigger the playback of the audio in a sample-accurate fashion. </p>
      
      
      <h4>Buffering </h4>
      <p>From:  <a href="http://lists.w3.org/Archives/Public/public-audio/2012JanMar/0006.html" class="external free" title="http://lists.w3.org/Archives/Public/public-audio/2012JanMar/0006.html">http://lists.w3.org/Archives/Public/public-audio/2012JanMar/0006.html</a></p>
      <dl>
        <dd>It would be nice to have something like AudioNode.onready(123.2, callback)</dd>
        <dd>if the browser is really sure to playback properly.</dd>
      </dl>
      
      
      <h4>Support for basic polyphony </h4>
      
      <p>A large number of simultaneous sources must be able to be played back simultaneously. As a guideline, the use-cases have identified that 32 [TODO: validate the number based on FMOD] simultaneous audio sources are required for typical music and gaming applications.</p>
      
      <h4>Multi-channel support</h4>
      <p>The API should support multi-channel (surround) sounds, in particular:</p>
      <ul>
        <li>Channel layouts and mapping should be supported. Mapping of typical layouts (mono, stereo, quad, 5.1 and 7.1) should be clearly specified.</li>
        <li>Channel up and down-mixing should be allowed. That means
        <ul>
          <li>The ability to match the number of channels supported by the hardware (with no upper limit?)</li>
          <li>The ability to up mix any source to match the number of channels of the system</li>
          <li>The ability to use as source an audio stream with more channels than the system supports</li>
          <li>The ability to down mix a source or stream to any number of channels, down to mono</li>
        </ul>
        </li>
      </ul>  
      <h4>Rapid scheduling of many independent sources </h4>
      <p>The ability to construct and schedule the playback of approximately 100 notes or sonic events per second across all voices would be required for typical music synthesis and gaming applications. 
      </p>
      <h4>Triggering of audio sources </h4>
      <p>It should be possible to trigger playback of audio sources in response to DOM events (onMouseOver, onKeyPress etc.), in addition it should be possible for the client to ascertain (for example using a callback) when an event has started and finished.
      </p><p>A conforming specification MUST be able to play pre-loaded sounds back at faster than a rate of 'x' milliseconds from the time the JavaScript code was executed, until the time that sound is heard through the speakers: where the audio path is not running through any external sound devices and the browser is using the default sound driver provided by the operating system.
      </p><p><i>Examples:</i>
      </p>
      <pre>
      document.addEventListener( 'keypress', function(){
        source.play();
      }, false );

      </pre>
      <h4>Audio quality </h4>
      <p>As a general requirement audio playback should be free of glitches, jitter and other distortions.
      </p>
      
      </section>
      <section>
      <h3>Transformations of sources of audio </h3>
      <p>Each of the sources of audio described above should be able to be transformed in real time. This processing should, as much as possible, have a low latency on a wide variety of target platforms. 
      </p>
      <h4>Modularity of transformations </h4>
      <p>The Audio Processing API should allow arbitrary combinations of transforms. A number of use-cases have the requirement that the developer has control over the transforms in a “modular” fashion.
      </p>
      <h4>Transformation parameter automation </h4>
      <p>Where there are parameters for these effects, it should be possible to automatically modify these parameters in a programmatic, time-dependent way. Parameter changes must be able to be scheduled relative to a source’s onset time which may be in the future. Primary candidates for automation include gain, playback rate and filter frequency.
      </p><p>Transformations include:
      </p>
      <h4>Gain adjustment </h4>
      <h4>Playback rate adjustment </h4>
      <h4>Spatialization </h4>
      <ul><li> equal power/level panning
      </li><li> binaural HRTF-based spatialization 
      </li><li> including the influence of the directivity of acoustic sources
      </li><li> including the attenuation of acoustic sources by distance
      </li><li> including the effect of movement on acoustic sources
      </li></ul>
      <h4>Filtering </h4>
      <ul><li> graphic EQ
      </li><li> low/hi/bandpass filters
      </li><li> impulse response filters
      </li><li> Pitch shifting
      </li><li> Time stretching
      </li></ul>
      <h4>Noise gating </h4>
      <p>It is possible to apply a real-time noise gate to a source to automatically mute it when its average power level falls below some arbitrary threshold (is this a reflexive case of ducking?).
      </p>
      <h4>Dynamic range compression </h4>
      <p><b>TBA</b>
      </p>
      <h4>The simulation of acoustic spaces </h4>
      <p>it should be possible to give the impression to the listener that a source is in a specific acoustic environment. The simulation of this environment may also be based on real-world measurements.
      </p>
      <h4>The simulation of occlusions and obstructions </h4>
      <p>It should be possible to give the impression to the listener that an acoustic source is occlusions or obstructed by objects in the virtual space
      </p>
      </section>
      <section>
        <h3>Source Combination and Interaction </h3>
      <h4>Mixing Sources </h4>
      <p>many independent sources can be mixed
      </p>
      <h4>Ducking </h4>
      <p>It is possible to mute or attenuate one source based on the average power level of another source, in real time.
      </p>
      <h4>Echo cancellation </h4>
      <p>It is possible to apply echo cancellation to a set of sources based on real-time audio input (say, a microphone).
      </p>
      </section>
      <section>
        <h3>Analysis of sources </h3>
      <h4>Level detection </h4>
      <p>A time-averaged volume or power level can be extracted from a source in real time (for visualisation or conversation-status purposes)
      </p>
      <h4>Frequency domain analysis </h4>
      <p>A frequency spectrum can be extracted from a source in real time (for visualisation purposes).
      </p>
      </section>
      <section>
        <h3>Synthesis of sources </h3>
      <h4>Generation of common signals for synthesis and parameter modulation purposes </h4>
      <p>For example sine, sawtooth, square and white noise. 
      </p>
      <h4>The ability to read in standard definitions of wavetable instruments </h4>
      <p>(e.g. Sound Font, DLS)
      </p>
      <h4>Acceptable performance of synthesis </h4>
      <p>TBD</p>

      <h2>Other Considerations </h2>
      <h4> Performance and Hardware-acceleration friendliness </h4>
      <ul><li> From WebRTC WG: audio-processing needs to be doable for real-time communications — this means getting processing, and using hardware-capabilities as much as possible.
      </li></ul>
      <ul><li> Improved performance and reliability to play, pause, stop and cache sounds, especially using the HTML5 Appcache offline caching for the HTML audio element.  From Paul Bakaus, Zynga (see <a href="http://lists.w3.org/Archives/Public/public-audio/2011AprJun/0128.html" title="http://lists.w3.org/Archives/Public/public-audio/2011AprJun/0128.html">thread</a>)
      </li></ul>
      </section>


    </section>
  <section>

      <h2>Mapping Use Cases and Requirements</h2>

      <table>
      <tr>
        <th style="width: 35em">Requirement Family</th>
        <th style="width: 55em">Requirement</th>
        <th style="width: 20em">Requirement Priority</th>
        <th>Video Chat</th>
        <th>HTML5 game with audio effects, music</th>
        <th>Online music production tool</th>
        <th>Online radio broadcast</th>
        <th>writing music on the web</th>
        <th>wavetable synthesis of a virtual music instrument</th>
        <th>Audio / Music Visualization</th>
        <th>UI/DOM Sounds</th>
        <th>UC-9 : Language learning</th>
        <th> UC-10 : Podcast on a flight</th>
        <th> UC-11: DJ music at 125 BPM</th>
        <th> UC-12 : Soundtrack and sound effects in a video editing tool</th>
        <th> UC-13 : Web-based guitar practice service</th>
        <th>UC-14 : User Control of Audio</th>
        <th>UC-15 : Video commentary</th>
      </tr>
      <tr>
        <th colspan="2">Use Case Priority</th>
        <td></td>
        <th>High</th>
        <th>High</th>
        <th>Low</th>
        <th>Low</th>
        <th>Low</th>
        <th>High</th>
        <th>High</th>
        <th>Low</th>
        <th>Low</th>
        <th>Low</th>
        <th>Low</th>
        <th>Low</th>
        <th>Low</th>
        <th>Low</th>
        <th>Low</th>
      </tr>
      <tr>
      <th rowspan='11'>Sources of audio</th>
        <td>Support for primary audio file formats</td>
        <td>Baseline</td>
    <!--    1            2            3            4            5            6            7            8            9            10           11          12           13           14         15    -->

        <td>✓</td>   <td>✓</td>   <td>✓</td>   <td>✓</td>   <td>✓</td>   <td>✓</td>   <td> </td>   <td>✓</td>   <td>✓</td>   <td>✓</td>   <td>✓</td>   <td>✓</td>   <td>✓</td>  <td> </td>  <td>✓</td>
      </tr>
      <tr>
        <td> One source, many sounds </td>
        <td>Minority, but important</td>
        <td> </td>   <td>✓</td>   <td>✓</td>   <td> </td>   <td>✓</td>   <td>✓</td>   <td> </td>   <td>✓</td>   <td> </td>   <td> </td>   <td> </td>   <td>✓</td>   <td> </td>  <td> </td>  <td> </td>
      </tr>
      <tr>
        <td> Playing / Looping sources of audio </td>
        <td>Baseline</td>
        <td> </td>   <td>✓</td>   <td>✓</td>   <td>✓</td>   <td>✓</td>   <td>✓</td>   <td>✓</td>   <td>✓</td>   <td>✓</td>   <td>✓</td>   <td>✓</td>   <td>✓</td>   <td>✓</td>  <td>✓</td>  <td>✓</td>
      </tr>
      <tr>
        <td>Capture of audio from microphone, line in, other inputs</td>
        <td>Minority, but important</td>
        <td>✓</td>   <td> </td>   <td> </td>   <td>✓</td>   <td> </td>   <td> </td>   <td> </td>   <td> </td>   <td> </td>   <td> </td>   <td> </td>   <td> </td>   <td>✓</td>  <td> </td>  <td>✓</td>
      </tr>
      <tr>
        <td>Adding effects to the audio part of a video stream, and keep it in sync with the video playback</td>
        <td>Minority, but important</td>
        <td>✓</td>   <td> </td>   <td> </td>   <td> </td>   <td> </td>   <td> </td>   <td> </td>   <td> </td>   <td> </td>   <td> </td>   <td> </td>   <td>✓</td>   <td> </td>  <td> </td>  <td>✓</td>
      </tr>
      <tr>
        <td> Sample-accurate scheduling of playback </td>
        <td>Minority, but important</td>
        <td> </td>   <td>✓</td>   <td>✓</td>   <td> </td>   <td>✓</td>   <td>✓</td>   <td> </td>   <td> </td>   <td> </td>   <td> </td>   <td> </td>   <td>✓</td>   <td> </td>  <td> </td>  <td> </td>
      </tr>
      <tr>
        <td> Buffering </td>
        <td>Minority, but important</td>
        <td> </td>   <td>✓</td>   <td> </td>   <td>✓</td>   <td> </td>   <td> </td>   <td>✓</td>   <td> </td>   <td>✓</td>   <td> </td>   <td> </td>   <td> </td>   <td>✓</td>  <td> </td>  <td> </td>
      </tr>
      <tr>
        <td> Multi-channel support</td>
        <td>Baseline</td>
        <td>✓</td>   <td>✓</td>   <td>✓</td>   <td>✓</td>   <td>✓</td>   <td>✓</td>   <td>✓</td>   <td>✓</td>   <td>✓</td>   <td>✓</td>   <td>✓</td>   <td>✓</td>   <td>✓</td>  <td>✓</td>  <td>✓</td>
      </tr>
      <tr>
        <td> Support for basic polyphony </td>
        <td>Baseline</td>
        <td>✓</td>   <td>✓</td>   <td>✓</td>   <td>✓</td>   <td>✓</td>   <td>✓</td>   <td> </td>   <td>✓</td>   <td> </td>   <td> </td>   <td>✓</td>   <td>✓</td>   <td> </td>  <td> </td>  <td> </td>
      </tr>
      <tr>

        <td> Rapid scheduling of many independent sources </td>
        <td>Minority, but important</td>
        <td> </td>   <td>✓</td>   <td>✓</td>   <td> </td>   <td>✓</td>   <td>✓</td>   <td> </td>   <td> </td>   <td> </td>   <td> </td>   <td> </td>   <td>✓</td>   <td> </td>  <td> </td>  <td> </td>
      </tr>
      <tr>
        <td> Triggering of audio sources </td>
        <td>Minority, but important</td>
        <td> </td>   <td>✓</td>   <td> </td>   <td>✓</td>   <td>✓</td>   <td>✓</td>   <td> </td>   <td>✓</td>   <td> </td>   <td> </td>   <td> </td>   <td> </td>   <td> </td>  <td> </td>  <td>✓</td>
      </tr>
      <tr>
        <td> Audio quality </td>
        <td>Baseline</td>
        <td>✓</td>   <td>✓</td>   <td>✓</td>   <td>✓</td>   <td>✓</td>   <td>✓</td>   <td> </td>   <td>✓</td>   <td>✓</td>   <td> </td>   <td>✓</td>   <td>✓</td>   <td>✓</td>  <td> </td>  <td> </td>
      </tr>
      <tr>

    <!--    1            2            3            4            5            6            7            8            9            10           11          12           13           14         15    -->


      <th rowspan='10'>Transformations of sources of audio </th>
        <td> Modularity of transformations </td>
        <td>Baseline</td>
        <td>✓</td>   <td> </td>   <td>✓</td>   <td>✓</td>   <td>✓</td>   <td>✓</td>   <td> </td>   <td> </td>   <td>✓</td>   <td> </td>   <td> </td>   <td>✓</td>   <td> </td>  <td> </td>  <td> </td>
      </tr>
      <tr>
        <td> Transformation parameter automation </td>
        <td>Baseline</td>
        <td>✓</td>   <td> </td>   <td>✓</td>   <td>✓</td>   <td>✓</td>   <td>✓</td>   <td> </td>   <td> </td>   <td>✓</td>   <td>✓</td>   <td> </td>   <td>✓</td>   <td> </td>  <td> </td>  <td> </td>
      </tr>
      <tr>
        <td> Gain adjustment </td>
        <td>Baseline</td>
        <td>✓</td>   <td>✓</td>   <td>✓</td>   <td>✓</td>   <td>✓</td>   <td>✓</td>   <td> </td>   <td>✓</td>   <td> </td>   <td> </td>   <td>✓</td>   <td>✓</td>   <td>✓</td>  <td>✓</td>  <td>✓</td>
      </tr>
      <tr>
        <td> Simple playback rate adjustment </td>
        <td>Baseline</td>
        <td> </td>   <td>✓</td>   <td>✓</td>   <td>✓</td>   <td>✓</td>   <td>✓</td>   <td> </td>   <td> </td>   <td>✓</td>   <td>✓</td>   <td>✓</td>   <td>✓</td>   <td> </td>  <td> </td>  <td> </td>
      </tr>
      <tr>
        <td> Spatialization </td>
        <td>Minority, but important</td>
        <td>✓</td>   <td>✓</td>   <td>✓</td>   <td> </td>   <td> </td>   <td> </td>   <td> </td>   <td>✓</td>   <td> </td>   <td> </td>   <td> </td>   <td> </td>   <td> </td>  <td> </td>  <td> </td>
      </tr>
      <tr>
        <td> Filtering </td>
        <td>Baseline</td>
        <td>✓</td>   <td> </td>   <td>✓</td>   <td>✓</td>   <td> </td>   <td>✓</td>   <td> </td>   <td> </td>   <td>✓</td>   <td>✓</td>   <td> </td>   <td>✓</td>   <td>✓</td>  <td> </td>  <td> </td>
      </tr>
      <tr>
        <td> Noise gating </td>
        <td>Minority, but important</td>
        <td>✓</td>   <td> </td>   <td> </td>   <td>✓</td>   <td> </td>   <td> </td>   <td> </td>   <td> </td>   <td> </td>   <td> </td>   <td> </td>   <td> </td>   <td>✓</td>  <td> </td>  <td> </td>
      </tr>
      <tr>
        <td> Dynamic range compression </td>
        <td><strong>Minority, but important</strong></td>
        <td> </td>   <td> </td>   <td>✓</td>   <td> </td>   <td> </td>   <td>✓</td>   <td> </td>   <td> </td>   <td> </td>   <td> </td>   <td> </td>   <td>✓</td>   <td>✓</td>  <td>✓</td>  <td>✓</td>
      </tr>
      <tr>
        <td> The simulation of acoustic spaces </td>
        <td>Minority, but important</td>
        <td>✓</td>   <td>✓</td>   <td> </td>   <td>✓</td>   <td> </td>   <td> </td>   <td> </td>   <td> </td>   <td> </td>   <td> </td>   <td> </td>   <td> </td>   <td> </td>  <td> </td>  <td> </td>
      </tr>
      <tr>
        <td>The simulation of occlusions and obstructions </td>
        <td>Minority, but important</td>
        <td> </td>   <td>✓</td>   <td> </td>   <td> </td>   <td> </td>   <td> </td>   <td> </td>   <td> </td>   <td> </td>   <td> </td>   <td> </td>   <td> </td>   <td> </td>  <td> </td>  <td> </td>
      </tr>

    <!--    1            2            3            4            5            6            7            8            9            10           11          12           13      -->

      <tr>
      <th rowspan='3'>Source Combination and Interaction </th>
        <td> Mixing Sources </td>
        <td>Baseline</td>
        <td>✓</td>   <td>✓</td>   <td>✓</td>   <td>✓</td>   <td>✓</td>   <td>✓</td>   <td> </td>   <td>✓</td>   <td> </td>   <td> </td>   <td> </td>   <td>✓</td>   <td> </td>  <td> </td>  <td> </td>
      </tr>
      <tr>
        <td> Ducking </td>
        <td>Minority, but important</td>
        <td>✓</td>   <td> </td>   <td>✓</td>   <td>✓</td>   <td> </td>   <td> </td>   <td> </td>   <td> </td>   <td> </td>   <td> </td>   <td>✓</td>   <td>✓</td>   <td> </td>  <td> </td>  <td>✓</td>
      </tr>
      <tr>
        <td> Echo cancellation </td>
        <td>Minority, but important</td>
        <td>✓</td>   <td> </td>   <td> </td>   <td>✓</td>   <td> </td>   <td> </td>   <td> </td>   <td> </td>   <td> </td>   <td> </td>   <td> </td>   <td> </td>   <td> </td>  <td> </td>  <td> </td>
      </tr>
      <tr>
      <th rowspan='2'>Analysis of sources </th>
        <td> Level detection </td>
        <td>Minority, but important</td>
        <td>✓</td>   <td> </td>   <td>✓</td>   <td>✓</td>   <td> </td>   <td> </td>   <td>✓</td>   <td> </td>   <td> </td>   <td> </td>   <td> </td>   <td>✓</td>   <td> </td>  <td> </td>  <td> </td>
      </tr>
      <tr>
        <td> Frequency domain analysis </td>
        <td>Minority, but important</td>
        <td> </td>   <td> </td>   <td> </td>   <td> </td>   <td> </td>   <td> </td>   <td>✓</td>   <td> </td>   <td> </td>   <td> </td>   <td>✓</td>   <td> </td>   <td> </td>  <td> </td>  <td> </td>
      </tr>
      <tr>
      <th rowspan='3'>Synthesis of sources</th>
        <td> Generation of common signals for synthesis and parameter modulation purposes </td>
        <td>Minority, but important</td>
        <td> </td>   <td>✓</td>   <td> </td>   <td> </td>   <td> </td>   <td>✓</td>   <td> </td>   <td> </td>   <td> </td>   <td> </td>   <td> </td>   <td> </td>   <td> </td>  <td> </td>  <td> </td>
      </tr>
      <tr>
        <td> The ability to read in standard definitions of wavetable instruments </td>
        <td>Minority, but important</td>
        <td> </td>   <td>✓</td>   <td> </td>   <td> </td>   <td> </td>   <td>✓</td>   <td> </td>   <td> </td>   <td> </td>   <td> </td>   <td> </td>   <td> </td>   <td> </td>  <td> </td>  <td> </td>
      </tr>
      <tr>
        <td> Acceptable performance of synthesis </td>
        <td>Minority, but important</td>
        <td> </td>   <td>✓</td>   <td> </td>   <td> </td>   <td> </td>   <td>✓</td>   <td> </td>   <td> </td>   <td> </td>   <td> </td>   <td> </td>   <td> </td>   <td> </td>  <td> </td>  <td> </td>
      </table>

    </section>

        <section>
        <h2>Features out of scope</h2>
        <p>During its lifetime, the W3C Audio working group also considered a number of features or requirements which were not deemed important enough to be kept in the scope of the first revision of the Web Audio API, but were worth recording for future perusal.</p>
        
        <h3>An AudioParam constructor in the context of a JavaScriptAudioNode</h3>
        <p>TBA: https://www.w3.org/2011/audio/track/issues/6</p>

</section>
    
    <section class='appendix'>
      <h2>Acknowledgements</h2>

      <p>This document is the result of the work of the W3C <a href="http://www.w3.org/2011/audio/">Audio
      Working Group</a>. Members of the working group, at the time of publication, included: </p>
      <ul>
        <li>Bateman, Adrian (Microsoft Corporation);</li>
        <li>Berkovitz, Joe (Invited expert);</li>
        <li>Cardoso, Gabriel (INRIA);</li>
        <li>Carlson, Eric (Apple, Inc.);</li>
        <li>Chen, Bin (Baidu, Inc.);</li>
        <li>Geelnard, Marcus (Opera Software);</li>
        <li>Goode, Adam (Google, Inc.);</li>
        <li>Gregan, Matthew (Mozilla Foundation);</li>
        <li>Jägenstedt, Philip (Opera Software);</li>
        <li>Kalliokoski, Jussi (Invited expert);</li>
        <li>Lowis, Chris (British Broadcasting Corporation);</li>
        <li>MacDonald, Alistair (Invited Expert);</li>
        <li>Mandyam, Giridhar (Qualcomm Innovation Center, Inc);</li>
        <li>Michel, Thierry (W3C/ERCIM);</li>
        <li>Noble, Jer (Apple, Inc.);</li>
        <li>O'Callahan, Robert (Mozilla Foundation);</li>
        <li>Olivier, Frank (Microsoft Corporation);</li>
        <li>Paradis, Matthew (British Broadcasting Corporation);</li>
        <li>Peraza Barreras, Jorge Armando (Microsoft Corporation);</li>
        <li>Raman, T.V. (Google, Inc.);</li>
        <li>Rogers, Chris (Google, Inc.);</li>
        <li>Schepers, Doug (W3C/MIT);</li>
        <li>Shires, Glen (Google, Inc.);</li>
        <li>Smith, Michael (W3C/Keio);</li>
        <li>Thereaux, Olivier (British Broadcasting Corporation);</li>
        <li>Wei, James (Intel Corporation);</li>
        <li>Wilson, Chris (Google,Inc.); </li>
        <li>Young, Milan (Nuance Communications, Inc.).</li>
      </ul>
      
      <p>The people who have contributed to <a href="http://lists.w3.org/Archives/Public/public-audio/">discussions on
      public-audio@w3.org</a> are also gratefully acknowledged. </p>
      
      <p>This document was also heavily influenced by earlier work by the audio working group and others, including:</p>
      <ul>
        <li>A list of “<a href="http://www.w3.org/2005/Incubator/audio/wiki/Audio_API_Use_Cases" title="Audio API Use Cases - Audio Incubator">Core Use Cases</a>” authored by the <a href="http://www.w3.org/2005/Incubator/audio/" title="W3C Audio Incubator Group">W3C Audio Incubator Group</a>, which predated the W3C Audio Working Group</li>
        <li> The <a href="http://tools.ietf.org/html/draft-ietf-rtcweb-use-cases-and-requirements-06#section-4.2" title="http://tools.ietf.org/html/draft-ietf-rtcweb-use-cases-and-requirements-06#section-4.2">use cases requirements from Web RTC</a></li>
        <li> The <a href="http://www.w3.org/TR/2011/WD-streamproc-20111215/#scenarios" title="http://www.w3.org/TR/2011/WD-streamproc-20111215/#scenarios">Scenarios from the Media Streams Processing</a></li> 
      </ul>
      
      
      
    </section>
  </body>
</html>

